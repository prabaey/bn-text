{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward neural network baseline\n",
    "\n",
    "We build two feed-forward neural networks, one for each diagnosis (pneu and inf), which take both symptoms and text embeddings as an input, and learn to output a prediction for the diagnosis probability. \n",
    "\n",
    "![title](figures/models_FF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn all the nan values into a third cateogry, \"unk\". This way, dysp, cough and nasal can take on three values: \"no\", \"yes\" and \"unk\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/train_4000_final.p\", \"rb\") as file: \n",
    "    train_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(\"unk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/test_1000_final.p\", \"rb\") as file: \n",
    "    test_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\"season\": {\"warm\": 0, \"cold\": 1}, \n",
    "            \"pneu\": {\"no\": 0, \"yes\": 1}, \"inf\": {\"no\": 0, \"yes\": 1}, \n",
    "            \"dysp\": {\"no\": 0, \"yes\": 1, \"unk\": 2}, \"cough\": {\"no\": 0, \"yes\": 1, \"unk\": 2}, \"nasal\": {\"no\": 0, \"yes\": 1, \"unk\": 2}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a Pytorch Dataset class that turns the background and symptom portion of all samples into a feature vector, while also storing the BioLORD embeddings as text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import itertools \n",
    "\n",
    "class TextSymptomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Feature vector dataset for use in Pytorch models\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, keeplen, class_map, emb_type, feature_names, interactions, device):\n",
    "        \"\"\"\n",
    "        dataframe: dataset containing samples made up of values for the background, disease and symptom variables, as well as text embeddings\n",
    "        keeplen: discard all samples past this index, except when building up the interaction features\n",
    "        class_map: dictionary mapping class names for each variable to index {\"var_name\": {\"class_name\": int}}\n",
    "        emb_type: name of embedding to use, will be \"BioLORD emb\" in our models\n",
    "        feature_names: names of categorical input features\n",
    "        interactions: whether to include extra features at the input capturing the inter-variable interactions between feature_names\n",
    "        device: device to load tensors to\n",
    "        \"\"\"\n",
    "\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self.dataframe = dataframe\n",
    "        self.keeplen = keeplen\n",
    "        self.class_map = class_map\n",
    "        self.emb_type = emb_type\n",
    "        self.device = device\n",
    "        self.feature_names = feature_names\n",
    "        self.interactions = interactions\n",
    "\n",
    "        self.preprocessed_data = self.preprocess()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" \n",
    "        returns number of samples in dataset\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.preprocessed_data[\"index\"])\n",
    "\n",
    "    def generate_one_hot(self, df):\n",
    "        \"\"\"\n",
    "        generate one-hot encodings for all samples in the dataframe\n",
    "        df: dataframe containing values for all variables at input \n",
    "\n",
    "        returns: dataframe with full background+symptom encoding for every sample \n",
    "                 number of columns = 2 + 3 + 3 + 3 + #pairwise interactions + #threeway interactions + #fourway interactions\n",
    "                 each column contains either 0 or 1\n",
    "        \"\"\"\n",
    "        \n",
    "        df = df.copy(deep=True)\n",
    "\n",
    "        # Feature-wise one-hot encodings\n",
    "        season_encoded = pd.get_dummies(df['season'], prefix='season') # dim 2\n",
    "        dysp_encoded = pd.get_dummies(df['dysp'], prefix='dysp') # dim 3\n",
    "        cough_encoded = pd.get_dummies(df['cough'], prefix='cough') # dim 3\n",
    "        nasal_encoded = pd.get_dummies(df['nasal'], prefix='nasal') # dim 3\n",
    "\n",
    "        # Concatenate feature-wise one-hot encodings to the original DataFrame\n",
    "        df_single = pd.concat([season_encoded, dysp_encoded, cough_encoded, nasal_encoded], axis=1) # dim 11\n",
    "\n",
    "        # If no interaction features requested, we output concatenated one-hot encodings\n",
    "        if not self.interactions:\n",
    "            return df_single\n",
    "        \n",
    "        # If interaction features requested, we create new features for all possible pairwise, three-way and four-way combinations of input values\n",
    "        # These features are subsequently one-hot encoded \n",
    "\n",
    "        # Pairwise interactions -> [season=no,dysp=no],[season=no,dysp=yes],[season=no,dysp=unk],...,[dysp=unk,cough=no],[dysp=unk,cough=yes],[dysp=unk,cough=unk]\n",
    "        pairwise_interactions = list(itertools.combinations(df.columns, 2))\n",
    "\n",
    "        df_pairwise = pd.DataFrame()\n",
    "        for interaction in pairwise_interactions:\n",
    "            interaction_name = f'{interaction[0]}_{interaction[1]}'\n",
    "            df[interaction_name] = df[interaction[0]].astype(str) + '_' + df[interaction[1]].astype(str)\n",
    "            df_encoded = pd.get_dummies(df[interaction_name], prefix=interaction_name)\n",
    "            df_pairwise = pd.concat([df_pairwise, df_encoded], axis=1) # pairwise feature encoding for every sample\n",
    "\n",
    "        # Three-way interactions -> [season=no,dysp=no,cough=no],[season=no,dysp=no,cough=yes],...,[dysp=unk,cough=unk,nasal=no],[dysp=unk,cough=unk,nasal=yes],[dysp=unk,cough=unk,nasal=unk]\n",
    "        three_way_interactions = list(itertools.combinations(df.columns[:4], 3))\n",
    "\n",
    "        df_threeway = pd.DataFrame()\n",
    "        for interaction in three_way_interactions:\n",
    "            interaction_name = f'{interaction[0]}_{interaction[1]}_{interaction[2]}'\n",
    "            df[interaction_name] = df[interaction[0]].astype(str) + '_' + df[interaction[1]].astype(str) + '_' + df[interaction[2]].astype(str)\n",
    "            df_encoded = pd.get_dummies(df[interaction_name], prefix=interaction_name)\n",
    "            df_threeway = pd.concat([df_threeway, df_encoded], axis=1) # three-way feature encoding for every sample\n",
    "\n",
    "        # Total configuration -> four-way interactions; treat all possible combinations of background and symptom values as a feature, one-hot encode this feature for every sample\n",
    "        total_configurations = list(itertools.product(df['season'].unique(), df['dysp'].unique(), df['cough'].unique(), df['nasal'].unique()))\n",
    "\n",
    "        for configuration in total_configurations:\n",
    "            config_name = f'season={configuration[0]}_dysp={configuration[1]}_cough={configuration[2]}_nasal={configuration[3]}'\n",
    "            df[config_name] = (df['season'] == configuration[0]) & (df['dysp'] == configuration[1]) & (df['cough'] == configuration[2]) & (df['nasal'] == configuration[3])\n",
    "\n",
    "        # One-hot encode the total configuration\n",
    "        total_config_encoded = pd.get_dummies(df.iloc[:, -len(total_configurations):].idxmax(axis=1), prefix='total_config')\n",
    "\n",
    "        # Concatenate all one-hot encodings for interaction features to the DataFrame\n",
    "        df_encoded = pd.concat([df_single, df_pairwise, df_threeway, total_config_encoded], axis=1)\n",
    "\n",
    "        return df_encoded\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        turn dataframe of length n into a dictionary of tensors \n",
    "        use class_map to map variable classes (e.g. \"no\"/\"yes\") to float (e.g. 0.0/1.0). use np.nan for unobserved symptom values\n",
    "        dict contains {\"season\": tensor(dim=n), \"pneu\": tensor(dim=n), \"inf\": tensor(dim=n), \"dysp\": tensor(dim=n), \"cough\":tensor(dim=n), \"nasal\":tensor(dim=n),\n",
    "                       \"sympt_features\":tensor(dim=(n,m)), \"text\":tensor(dim=(n,768))} \n",
    "        with m = #features in symptom encoding (output of generate_one_hot)\n",
    "        \"\"\"\n",
    "        \n",
    "        preprocessed_data = {}\n",
    "\n",
    "        for var in self.class_map: \n",
    "            values = self.dataframe[var].apply(lambda x: self.class_map[var][x] if not pd.isna(x) else np.nan).values\n",
    "            preprocessed_data[var] = torch.tensor(values, dtype=torch.float, device=self.device)\n",
    "\n",
    "        preprocessed_data[\"text\"] = torch.tensor(self.dataframe[self.emb_type].tolist(), dtype=torch.float, device=self.device)\n",
    "        preprocessed_data[\"index\"] = torch.tensor(self.dataframe.index.tolist(), device=self.device)\n",
    "\n",
    "        df = self.dataframe.replace(self.class_map)\n",
    "        df_encoded = self.generate_one_hot(df[self.feature_names])\n",
    "        preprocessed_data[\"sympt_features\"] = torch.tensor(df_encoded.to_numpy(), dtype=torch.float, device=self.device)\n",
    "\n",
    "        if self.keeplen != -1: \n",
    "            preprocessed_data = {key: val[:self.keeplen] for key, val in preprocessed_data.items()}\n",
    "\n",
    "        return preprocessed_data\n",
    "    \n",
    "    def len_sympt_obs(self): \n",
    "        \"\"\"\n",
    "        get the number of samples in the dataset where the symptoms are observed (not nan)\n",
    "        \"\"\"\n",
    "        \n",
    "        df_sympt_obs = self.dataframe.dropna(axis=0, how=\"any\") # drop all records where symptoms not observed\n",
    "        return len(df_sympt_obs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        return sample {\"season\": tensor(dim=1), \"pneu\": tensor(dim=1), \"inf\": tensor(dim=1), \"dysp\": tensor(dim=1), \"cough\":tensor(dim=1), \"nasal\":tensor(dim=1),\n",
    "                       \"sympt_features\":tensor(dim=m), \"text\":tensor(dim=768)} \n",
    "        with m = #features in symptom encoding (output of generate_one_hot)\n",
    "        \"\"\"\n",
    "\n",
    "        dp = {var: self.preprocessed_data[var][index] for var in self.preprocessed_data.keys()}\n",
    "        return dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we encode the train set with feature interactions turned on. Note that the \"symp_features\" input vector has 101 dimensions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paloma\\AppData\\Local\\Temp\\ipykernel_22832\\3452574685.py:119: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  preprocessed_data[\"text\"] = torch.tensor(self.dataframe[self.emb_type].tolist(), dtype=torch.float, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "emb_type = \"BioLORD emb\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # put train and test data on the device\n",
    "feature_names = [\"season\", \"dysp\", \"cough\", \"nasal\"]\n",
    "interactions=True\n",
    "keeplen=-1\n",
    "train_set = TextSymptomDataset(train_df, keeplen, class_map, emb_type, feature_names, interactions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'season': tensor(1.),\n",
       " 'pneu': tensor(0.),\n",
       " 'inf': tensor(1.),\n",
       " 'dysp': tensor(0.),\n",
       " 'cough': tensor(1.),\n",
       " 'nasal': tensor(1.),\n",
       " 'text': tensor([ 3.5453e-01,  3.6790e-01, -1.1390e-02,  2.3277e-01, -7.3408e-02,\n",
       "          3.7717e-01,  5.5223e-02,  3.7554e-02, -3.3992e-01, -1.9111e-01,\n",
       "         -1.1521e-01,  7.4201e-02,  1.6542e-01,  3.5197e-01, -1.3381e-02,\n",
       "         -3.1276e-01,  2.1234e-02, -2.5756e-01, -1.4279e-01, -1.1621e-01,\n",
       "         -9.7627e-02, -5.7466e-02, -2.5728e-01, -9.0918e-02,  1.2057e-02,\n",
       "         -1.3028e-01,  1.0703e-01,  1.3443e-02, -1.0396e-01,  6.8164e-02,\n",
       "          1.4590e-01,  2.4242e-01,  2.0206e-01, -4.7323e-01, -2.2851e-01,\n",
       "         -4.0334e-02, -1.6823e-01,  8.0301e-03, -2.6753e-01, -6.9912e-02,\n",
       "         -3.0163e-03, -1.0121e-01, -2.9026e-02,  7.2016e-02,  5.0150e-02,\n",
       "          7.1253e-02,  1.2259e-01,  1.0259e-01, -9.9757e-02,  3.0487e-02,\n",
       "         -1.9827e-01, -2.2960e-01, -3.2041e-01, -8.0531e-02,  2.2730e-01,\n",
       "         -1.0927e-01,  1.7954e-01, -1.7577e-01,  1.3201e-01, -1.6068e-01,\n",
       "         -3.4078e-01, -2.7551e-01, -8.6188e-02, -1.0585e-01,  1.2679e-01,\n",
       "         -7.9782e-02, -2.3053e-01,  2.3870e-01, -6.1364e-02,  1.7089e-01,\n",
       "         -3.0564e-01, -2.4351e-01, -9.5180e-02,  3.0112e-01, -1.1122e-01,\n",
       "          3.7152e-01, -2.2101e-01, -7.3381e-02, -8.0268e-02,  1.4895e-02,\n",
       "         -5.2889e-02, -2.2410e-01,  1.8167e-01,  2.2204e-01,  2.5659e-02,\n",
       "         -6.6660e-02, -2.4814e-02, -1.9055e-01,  1.1991e-02,  1.0089e-01,\n",
       "         -2.1793e-01,  7.9537e-02, -1.8015e-01,  5.7939e-02, -1.0593e-01,\n",
       "         -2.7603e-01, -9.1454e-02,  1.3222e-01,  1.3313e-01, -2.5354e-01,\n",
       "         -6.2350e-02,  3.2205e-01,  2.7034e-01,  1.8016e-01, -2.9199e-01,\n",
       "         -1.5523e-01, -1.7099e-01,  2.6144e-01,  3.6131e-02,  2.1336e-01,\n",
       "         -2.9092e-01,  3.9850e-01, -1.5414e-01,  2.0896e-01, -9.2940e-02,\n",
       "          1.8926e-02,  8.4750e-02, -1.4172e-01,  8.7984e-02,  3.3653e-01,\n",
       "          2.8105e-01,  5.4747e-02, -3.1462e-01, -2.9614e-01, -8.2847e-02,\n",
       "          1.6952e-01,  1.8442e-01, -1.1387e-01, -3.9621e-01,  1.0941e-01,\n",
       "         -5.1030e-01, -9.0835e-02,  4.2275e-01, -7.9210e-03, -3.0125e-01,\n",
       "          2.5238e-01,  2.4614e-01,  4.7591e-02, -2.9180e-02,  6.8485e-02,\n",
       "         -9.2124e-02,  8.4813e-02, -1.0349e-01,  3.5239e-02,  8.8178e-02,\n",
       "          1.1368e-01,  3.5384e-01,  4.8439e-02, -2.4771e-01, -2.8287e-02,\n",
       "          5.2108e-02, -1.5138e-01, -2.6694e-01,  1.0425e-01,  1.9158e-01,\n",
       "          9.7450e-02, -1.0843e-01, -2.7395e-01,  1.3508e-01,  1.0484e-01,\n",
       "          3.5902e-01, -6.6080e-02, -1.0132e-01, -1.8875e-02,  3.0463e-01,\n",
       "         -9.9851e-02,  1.6795e-02,  1.2442e-01,  5.5327e-02, -2.0515e-01,\n",
       "         -1.9098e-01, -1.2340e-01, -1.4942e-02, -1.3361e-01, -1.7691e-02,\n",
       "          6.5861e-02, -2.7413e-01,  1.5903e-01,  7.2137e-02,  1.5668e-01,\n",
       "         -1.9992e-01, -7.0908e-02,  4.1330e-02,  4.2647e-01, -2.1122e-01,\n",
       "          3.2649e-02,  8.3967e-02, -1.1145e-01, -3.7074e-03,  1.7766e-01,\n",
       "          6.4875e-02, -1.9344e-01, -9.1840e-02,  1.3866e-01,  2.3898e-02,\n",
       "         -1.5415e-01,  1.2587e-01, -5.8417e-02, -1.1193e-01,  1.8976e-01,\n",
       "         -1.1085e-02, -2.5624e-01,  4.4808e-02, -2.4735e-01, -2.8076e-01,\n",
       "          1.1971e-01, -7.6587e-02, -3.3859e-02,  2.9819e-01, -1.2275e-01,\n",
       "         -1.6544e-01,  3.1196e-01,  2.2103e-02,  2.1225e-02, -7.8590e-02,\n",
       "         -8.0930e-02, -2.4629e-01, -2.6439e-01, -3.6254e-01,  5.9176e-02,\n",
       "         -8.3651e-02,  1.0802e-02, -2.3960e-01,  2.9328e-02,  2.0519e-01,\n",
       "         -1.8491e-01, -2.0869e-01,  1.7046e-02,  1.0401e-01, -2.3224e-01,\n",
       "          1.5736e-01,  2.3243e-01,  4.6544e-02, -2.3293e-01, -2.7925e-02,\n",
       "          3.4427e-03, -2.1725e-02, -5.8574e-01,  1.9305e-02, -3.7166e-02,\n",
       "         -3.1743e-01,  1.8981e-02,  2.1259e-01,  1.3630e-01,  4.0875e-01,\n",
       "          8.9500e-02,  1.7956e-01,  1.1513e-01, -5.0199e-02,  1.1055e-01,\n",
       "         -4.1009e-01, -1.8846e-02,  6.0789e-02,  1.0445e-01, -1.8832e-01,\n",
       "          1.0148e-01, -1.1067e-01, -2.7266e-02,  3.2431e-02, -3.1137e-02,\n",
       "         -1.2072e-01,  2.2676e-01, -2.8368e-01, -2.7638e-01,  2.5845e-01,\n",
       "          1.8482e-01, -1.1559e-01, -3.1026e-03, -9.7040e-02, -3.1142e-01,\n",
       "         -1.7135e-02,  4.1237e-01,  2.1618e-01, -1.4768e-01,  1.7287e-01,\n",
       "         -2.2590e-01, -1.3349e-01, -1.3160e-01, -4.6348e-02, -1.5322e-02,\n",
       "          1.8857e-01,  1.1176e-01,  3.2154e-02, -1.6789e-02,  2.1697e-01,\n",
       "         -2.7083e-01, -1.5103e-01, -2.2249e-01,  1.8010e-01, -5.5030e-02,\n",
       "         -7.3055e-02, -2.5246e-01,  7.3147e-02,  5.3300e-04, -1.1856e-01,\n",
       "         -2.6834e-01, -6.7349e-02,  8.8205e-02, -1.3083e-01,  4.1980e-02,\n",
       "         -2.6988e-01,  3.0102e-01, -2.6136e-01,  6.3222e-02,  1.6551e-01,\n",
       "          4.1818e-02,  2.5825e-02,  1.1203e-01,  3.1036e-01,  3.3345e-02,\n",
       "          1.1302e-01,  3.8227e-01,  1.6769e-02, -1.3812e-01, -1.4337e-01,\n",
       "          5.0509e-02,  7.6248e-02, -3.1781e-01,  9.9012e-02, -1.9032e-01,\n",
       "          1.6165e-03,  4.0663e-02,  2.4190e-01, -9.8210e-02, -1.9107e-01,\n",
       "          5.5659e-02,  7.5502e-02, -5.6736e-02, -2.1668e-01,  4.0505e-01,\n",
       "          9.6698e-02, -2.9568e-01, -2.3784e-01,  1.1621e-01,  5.0365e-02,\n",
       "         -2.9214e-01,  2.5866e-01, -1.4724e-01,  7.5069e-02,  2.9555e-01,\n",
       "          1.6998e-01, -1.6735e-01,  3.1496e-02, -2.3052e-01,  5.0173e-01,\n",
       "         -1.0507e-01, -3.6688e-01,  2.1357e-01, -3.9557e-01,  1.3965e-01,\n",
       "         -2.5911e-02,  2.5235e-01,  1.8337e-02,  1.1611e-01,  1.4329e-01,\n",
       "         -6.2523e-02,  1.0224e-01, -1.0710e-01, -1.1703e-01, -2.6211e-01,\n",
       "         -2.5873e-01, -6.2080e-03,  2.4260e-01,  7.3553e-02,  2.5776e-01,\n",
       "          4.7017e-01,  8.2153e-02, -7.8842e-02,  9.1071e-02,  1.6872e-01,\n",
       "         -2.1573e-01,  1.8214e-01,  6.8515e-02, -1.7732e-01,  6.3978e-02,\n",
       "         -2.1020e-02,  3.9383e-01,  1.0212e-01,  1.9759e-01, -2.9461e-01,\n",
       "         -1.7824e-02, -4.1042e-02, -5.9446e-01, -1.5486e-01,  1.2561e-01,\n",
       "         -1.1280e-02,  2.1837e-02,  2.0173e-01, -4.2626e-01,  7.5637e-02,\n",
       "         -6.0073e-02,  3.1189e-01, -3.7038e-02,  1.0440e-01, -1.1071e-02,\n",
       "          2.5055e-01,  3.4735e-01,  1.2962e-01,  7.9457e-02,  2.3082e-01,\n",
       "          2.6410e-01, -3.5070e-01, -2.1894e-01,  3.4968e-01, -6.3264e-02,\n",
       "          2.4269e-03,  2.4787e-01,  2.4161e-02,  7.6698e-02,  4.0328e-02,\n",
       "          3.1056e-01,  2.0069e-01,  2.2291e-01, -2.6706e-01, -1.0366e-02,\n",
       "          4.1216e-02,  1.7292e-01,  1.4571e-01, -4.8523e-02, -1.1070e-01,\n",
       "          1.3195e-01, -1.2884e-01,  1.8160e-01,  1.5636e-01, -1.3813e-02,\n",
       "         -3.3462e-01,  1.5355e-01,  4.5782e-01,  1.9836e-01, -1.5070e-01,\n",
       "          1.3854e-01,  2.5364e-01, -1.4047e-01,  1.2342e-02, -2.1312e-01,\n",
       "         -2.6690e-01,  6.4529e-02,  4.2968e-01,  4.1680e-02,  1.7858e-01,\n",
       "         -1.7987e-01, -2.8158e-01, -3.1931e-01,  2.5799e-03, -1.1842e-01,\n",
       "         -1.7817e-01,  2.3399e-01,  1.6818e-01,  3.0623e-01, -1.2889e-01,\n",
       "         -1.0129e-01,  1.1306e-01,  2.0886e-01, -4.3113e-01, -1.3555e-02,\n",
       "          1.1628e-01, -2.3193e-01, -1.5986e-01,  2.8001e-01, -1.4998e-01,\n",
       "         -7.9115e-02,  5.8955e-02,  4.6927e-01,  7.4173e-02,  1.1078e-01,\n",
       "          3.3413e-02,  1.1127e-01,  2.0096e-02,  2.1971e-01, -7.9221e-02,\n",
       "         -2.0810e-01,  8.7154e-02,  2.8339e-02, -3.8882e-02,  1.8434e-01,\n",
       "          1.9968e-01,  1.6928e-02,  1.3548e-01, -2.2050e-01,  1.6492e-01,\n",
       "         -6.9986e-02,  2.1711e-01,  3.4829e-01, -3.3052e-01, -1.0631e-01,\n",
       "         -9.4596e-02,  1.6169e-01, -1.7740e-01, -1.9973e-01,  2.4977e-01,\n",
       "          3.3193e-01, -4.7591e-03, -1.2158e-01, -2.3189e-01, -7.0197e-02,\n",
       "          2.1520e-01,  4.3404e-02,  1.5317e-01,  1.4611e-01, -1.2764e-01,\n",
       "          7.3947e-02, -2.3528e-02,  1.5286e-01,  1.3293e-01,  8.2154e-02,\n",
       "         -9.9368e-02, -1.5267e-01, -1.1870e-01, -5.7359e-02,  1.5035e-01,\n",
       "         -7.4538e-02,  1.2810e-01, -5.4249e-02, -3.1135e-01, -3.1905e-02,\n",
       "          2.3292e-01,  5.2009e-02, -3.3547e-02,  4.1515e-01, -2.5588e-02,\n",
       "          2.4098e-01, -2.2775e-01,  8.1341e-02,  1.7191e-01,  6.7927e-02,\n",
       "          1.5218e-01, -7.3147e-02, -1.6234e-02, -5.5122e-03, -6.0455e-02,\n",
       "         -4.4521e-01,  1.3479e-01,  3.0898e-01,  1.1755e-01, -1.3372e-01,\n",
       "          3.8080e-01, -3.0800e-01, -6.0514e-02, -2.2340e-01,  8.2652e-02,\n",
       "         -1.6975e-01,  2.5707e-02,  9.2624e-02,  8.6307e-02,  3.5003e-01,\n",
       "         -1.9083e-01, -1.1000e-01, -3.1578e-01,  1.3406e-01, -3.2197e-01,\n",
       "          8.8197e-02, -5.9276e-02, -2.1432e-01, -1.0452e-01, -1.6148e-01,\n",
       "          6.6996e-01,  7.0981e-02,  3.4554e-02,  8.4679e-03,  7.6339e-02,\n",
       "         -3.4672e-02, -1.7873e-01, -1.9904e-01,  1.7634e-01, -6.9091e-02,\n",
       "         -2.7063e-02, -1.2087e-01, -1.4095e-01,  1.1321e-01,  3.5281e-01,\n",
       "         -3.4471e-04,  7.2020e-02, -1.3735e-01,  1.0340e-01, -6.1488e-02,\n",
       "          1.2420e-01,  3.1411e-01, -1.7430e-01, -3.7749e-02,  1.9106e-01,\n",
       "         -1.1765e-01,  2.4716e-02,  6.7583e-04, -3.3274e-03,  1.0016e-01,\n",
       "         -2.9834e-01,  5.1882e-02,  1.0290e-01,  9.9754e-02,  9.2403e-02,\n",
       "         -1.3819e-01, -2.5710e-01, -1.0122e-02,  7.4555e-04, -2.0839e-01,\n",
       "         -8.4454e-02, -1.1217e-01,  2.2040e-01, -3.0101e-01, -1.1026e-01,\n",
       "          3.3194e-01,  1.8781e-01,  7.2344e-02,  2.3728e-02,  2.2714e-01,\n",
       "         -8.1850e-02, -3.7457e-01,  7.2388e-02, -4.6560e-01,  4.4974e-01,\n",
       "         -3.9638e-01, -4.6859e-02,  3.5660e-02,  1.6928e-01, -6.9447e-02,\n",
       "          2.6667e-01,  5.8311e-02, -1.1603e-02, -1.0783e-01, -2.4662e-01,\n",
       "         -1.4318e-01, -1.9071e-01, -7.5674e-01,  7.5609e-02, -2.1693e-01,\n",
       "         -7.1907e-02, -2.7357e-01,  6.3347e-03,  1.9856e-01, -3.4125e-01,\n",
       "          1.4370e-01, -1.7045e-01, -8.2027e-02,  3.1129e-01,  1.6362e-01,\n",
       "          8.1185e-02, -2.0124e-01,  1.7525e-01, -7.2696e-02, -1.1280e-01,\n",
       "         -5.0286e-02,  8.0960e-02, -1.6194e-02, -1.9650e-02, -7.6663e-02,\n",
       "          2.2252e-01, -3.0507e-01,  6.8872e-02, -1.7518e-01, -6.5409e-02,\n",
       "         -6.3142e-03, -1.0887e-01,  1.2296e-02, -1.5311e-01, -2.7444e-01,\n",
       "         -9.0762e-02,  6.4572e-02,  3.1640e-01, -1.1430e-01,  3.9875e-02,\n",
       "         -2.9186e-02, -9.8975e-02, -3.9414e-02, -2.0199e-01,  3.2926e-01,\n",
       "         -3.3958e-02,  1.8935e-01, -2.9333e-01, -2.7601e-02, -1.3249e-02,\n",
       "         -2.6823e-03, -3.1560e-01, -7.2624e-03, -1.3268e-01, -2.0200e-02,\n",
       "         -3.8596e-02,  2.5201e-01,  1.9996e-02,  1.3000e-01,  2.9879e-02,\n",
       "         -3.5021e-01,  1.6062e-03, -2.4980e-01, -2.9756e-01,  1.2407e-01,\n",
       "         -1.1472e-01,  2.1017e-01, -8.1484e-03, -4.4166e-01, -1.9119e-01,\n",
       "         -6.3488e-02,  1.6849e-02, -4.6629e-01,  1.8716e-01,  7.1840e-02,\n",
       "         -2.3645e-02, -1.0574e-01,  8.3225e-02, -1.8106e-01,  2.3189e-01,\n",
       "          9.8365e-02,  9.1218e-02, -1.2228e-01, -3.6981e-02,  2.0364e-01,\n",
       "          2.1979e-01,  1.0189e-01,  1.5558e-02, -1.7694e-01, -9.9087e-02,\n",
       "         -2.6867e-01, -1.3328e-01,  3.6814e-01,  8.1851e-02, -2.8236e-01,\n",
       "          3.0119e-01,  6.9235e-02,  1.5387e-01, -3.4343e-02, -7.8612e-02,\n",
       "         -3.4148e-01, -1.0022e-02, -1.6637e-01, -2.3778e-01, -8.3158e-03,\n",
       "         -3.0293e-02,  8.1012e-02,  1.8050e-01, -1.6152e-01, -3.4253e-01,\n",
       "          2.8584e-02, -1.3670e-02,  8.8905e-02,  1.9034e-01, -1.6956e-01,\n",
       "          1.1065e-02,  2.9655e-01,  1.1567e-01, -9.1447e-02,  1.3229e-03,\n",
       "         -5.5386e-02,  2.3158e-01,  4.5684e-02, -4.0202e-02,  5.1934e-02,\n",
       "          1.5119e-02, -1.3489e-01,  4.7823e-02,  1.1252e-01, -1.1042e-01,\n",
       "          2.4487e-02,  8.6849e-02, -1.7152e-01, -2.3455e-02, -3.3591e-01,\n",
       "         -8.3214e-02,  5.6239e-01,  5.1904e-02,  1.2519e-01,  2.2982e-01,\n",
       "         -2.7909e-01, -5.2633e-02, -1.2442e-01,  1.7413e-01,  1.9830e-01,\n",
       "          6.8658e-02, -8.8798e-02, -2.1370e-01]),\n",
       " 'index': tensor(0),\n",
       " 'sympt_features': tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set.__getitem__(0)[\"sympt_features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that the test set is encoded in the same interaction features. However, the symptoms are never unobserved in the test set, so if we would input it into the \"TextSymptomDataset\" class, we would be missing some combinations, and the feature representations between train and test set would not match up. For this reason, we add all possible combinations of the background and symptom variables to the test set, so these are also used to build up the interaction features, and then use the keeplen argument to ensure that only real test samples are retained in the final dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_text_emb = train_df[train_df[\"text\"] == \"\"].iloc[0][emb_type] # empty text embedding for evaluation purposes\n",
    "unk_entries = pd.DataFrame({\"season\": [\"warm\", \"cold\"], \"dysp\": [\"unk\", \"unk\"], \"cough\": [\"unk\", \"unk\"], \"nasal\": [\"unk\", \"unk\"], \"BioLORD emb\": 2*[empty_text_emb]})\n",
    "all_combos = pd.DataFrame({\"season\": [\"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\"], \n",
    "                           \"dysp\": [\"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"],\n",
    "                           \"cough\": [\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\"],\n",
    "                           \"nasal\": [\"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\"], \"BioLORD emb\": 16*[empty_text_emb]})\n",
    "test_df_ext = pd.concat([test_df, unk_entries, all_combos], axis=0) # ensure correct encoding (test set does not normally contain nans) -> are removed again in TextSymptomDataset\n",
    "test_set = TextSymptomDataset(test_df_ext, 1000, class_map, emb_type, [\"season\", \"dysp\", \"cough\", \"nasal\"], interactions=interactions, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set.__getitem__(0)[\"sympt_features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from utils.models import TextEmbClassifier\n",
    "\n",
    "def train_diag_classifier(train, test, diag_name, n_emb, hidden_dim, dropout, bs_train=100, epochs=100, seed=2023, lr=0.0001, weight_decay=1e-5):\n",
    "    \"\"\"\n",
    "    training loop for diagnosis classifier\n",
    "    \n",
    "    train: set of training samples\n",
    "    test: set of test samples\n",
    "    diag_name: name of output variable (pneu or inf) to learn to classify\n",
    "    n_emb: embedding size of text \n",
    "    hidden_dim: list of hidden dimensions to use in TextEmbClassifier\n",
    "    dropout: list of dropout probabilities to use in TextEmbClassifier\n",
    "    \n",
    "    bs_train: batch size to use for training\n",
    "    epochs: number of epochs to train for\n",
    "    seed: seed to use for initialization\n",
    "    lr: learning rate \n",
    "    weight_decay: L2 penalty parameter\n",
    "\n",
    "    returns\n",
    "        train_loss: list of train losses across epochs\n",
    "        test_loss: list of test losses across epochs\n",
    "        model: trained classifier for diag_name\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=bs_train, shuffle=True)\n",
    "    test_loader = DataLoader(test, batch_size=len(test), shuffle=False)\n",
    "\n",
    "    # put model on the device\n",
    "    model = TextEmbClassifier(n_emb, hidden_dim, dropout, seed)\n",
    "    model.to(device)\n",
    "\n",
    "    adam = Adam(params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss = torch.nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, x in enumerate(train_loader): \n",
    "\n",
    "            model.train() # put model in train mode\n",
    "            adam.zero_grad()\n",
    "\n",
    "            input = torch.cat((x[\"text\"], x[\"sympt_features\"]), dim=1) # concatenate text and symptom features at input\n",
    "\n",
    "            pred = model(input).squeeze() # predictions of model\n",
    "            batch_loss = loss(pred, x[diag_name]).sum()\n",
    "            batch_loss.backward()\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_value_(model.parameters(), 5)\n",
    "            adam.step()\n",
    "        \n",
    "        train_loss.append(epoch_loss/len(train))\n",
    "\n",
    "        model.eval() # put model in eval mode\n",
    "        with torch.no_grad():\n",
    "            for x_test in test_loader: \n",
    "\n",
    "                input = torch.cat((x_test[\"text\"], x_test[\"sympt_features\"]), dim=1)\n",
    "\n",
    "                pred = model(input).squeeze()\n",
    "                batch_loss = loss(pred, x_test[diag_name]).sum()\n",
    "                test_loss.append(batch_loss.item()/len(test))\n",
    "\n",
    "    return train_loss, test_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def predict_diagnoses(model, diag, test_set, unk_features, bs, excl_text=False, excl_sympt=False, empty_text_emb=None): \n",
    "\n",
    "    \"\"\" \n",
    "    build prediction dataframe for diagnosis (pneu/inf) based on predictions made by trained model for set of test samples\n",
    "\n",
    "    model: trained diagnosis classifier\n",
    "    diag: diagnosis to predict \n",
    "    test_set: dataframe with test cases\n",
    "    unk_features: feature representation of sample with unknown symptoms, to use at input when excl_sympt=True\n",
    "                  dimension (2, 101) (contains representation for both season=0 and season=1)\n",
    "    bs: batch size to use when looping over test cases\n",
    "    excl_text: if True, use empty text embedding in input vector, calculate P(diag=yes|background,symptoms)\n",
    "    excl_sympt: if True, use unknown symptom representation in input vector, calculate P(diag=yes|background,text)\n",
    "                if excl_text and excl_sympt are both False, we calculate P(diag=yes|background,symptoms,text)\n",
    "    empty_text_emb: embedding of empty text \"\" to use at input when excl_text=True\n",
    "\n",
    "    returns: test_set dataframe, extended with prediction for diagnosis (pred_pneu/pred_inf)\n",
    "    \"\"\"\n",
    "\n",
    "    test_loader = DataLoader(test_set, batch_size=bs, shuffle=True)\n",
    "    res_df = pd.DataFrame(columns=list(test_set.__getitem__(0).keys()).remove(\"sympt_features\"))\n",
    "    res_df = res_df.rename({\"text\": \"emb\"}, axis=1)\n",
    "\n",
    "    model.eval() # set model in eval mode\n",
    "\n",
    "    for x in test_loader: \n",
    "\n",
    "        with torch.no_grad():\n",
    "            if excl_text: # P(Diag=yes|background,symptoms)\n",
    "                x[\"text\"] = empty_text_emb.unsqueeze(0).expand(x[\"text\"].shape[0], -1) # set all embeddings to empty \n",
    "            if excl_sympt:  # P(Diag=yes|background,text)\n",
    "                            # set to encoding corresponding to unknown features, while background remains known\n",
    "                if not test_set.interactions: \n",
    "                    unk_features = unk_features[:, :11] # if no interactions, only select individual features\n",
    "                unk_exp = unk_features[None, :].expand(x[\"season\"].shape[0], -1, -1) # shape (bs, 2, 101) or (bs, 2, 11)\n",
    "                x_exp = x[\"season\"][:, None].expand(-1, unk_features.shape[-1])[:, None, :] # shape (bs, 1, 101) or (bs, 1, 11)\n",
    "                x[\"sympt_features\"] = torch.gather(unk_exp, 1, x_exp.long()).squeeze(1) # select feature representation of unknown features based on value of background variable\n",
    "                                                                                        # shape (bs, 101) or (bs, 11)\n",
    "\n",
    "            input = torch.cat((x[\"text\"], x[\"sympt_features\"]), dim=1)\n",
    "            res_diag = model(input) # P(Diag=yes|background,symptoms,text)\n",
    "\n",
    "        batch_df = pd.DataFrame({key:val.cpu().numpy() for key, val in x.items() if key != \"text\" and key != \"sympt_features\"})\n",
    "        batch_df[f\"pred_{diag}\"] = res_diag.cpu().numpy()\n",
    "        batch_df[\"emb\"] = list(x[\"text\"].cpu())\n",
    "        res_df = pd.concat([res_df, batch_df], ignore_index=True)\n",
    "\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the train loop and calculate the average precision of the following predictions on the test set:\n",
    "- P(Diag=yes|background,symptoms,text)\n",
    "- P(Diag=yes|background,symptoms) (input empty text embedding)\n",
    "- P(Diag=yes|background,text) (input feature representation of unknown symptoms)\n",
    "\n",
    "All predictions are calculated via the function above. \n",
    "\n",
    "We optimize the following parameters by maximizing average precision over P(Diag=yes|background,symptoms,text) on a validation split:\n",
    "- dropout\n",
    "- bs_train\n",
    "- epochs\n",
    "- lr\n",
    "- weight_decay\n",
    "- hidden_dim (separately for \"pneu\" and \"inf\")\n",
    "- interactions (separately for \"pneu\" and \"inf\")\n",
    "\n",
    "We used the following validation split to optimize these parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 2024\n",
    "subtrain_df, val_df = train_test_split(train_df, train_size=0.8, random_state=seed) # use val_df in code below to optimize hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pieces of code show the final hyperparameters chosen for the \"pneu\" and \"inf\" classifier respectively. We train the models on the full train set and evaluate on the test set. The final results reported in the paper were obtained by running this code over 5 initialization seeds [422, 957, 267, 956, 781]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import performance_metrics\n",
    "\n",
    "seeds = [422, 957, 267, 956, 781]\n",
    "\n",
    "# hyperparameter settings\n",
    "interactions = True\n",
    "diag_name = \"inf\"\n",
    "if interactions:\n",
    "    n_emb = 768+101\n",
    "else: \n",
    "    n_emb = 768+11\n",
    "hidden_dim = [1]\n",
    "dropout = 0.7\n",
    "bs_train = 256\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# extend test set with all possible symptom combinations\n",
    "empty_text_emb = train_df[train_df[\"text\"] == \"\"].iloc[0][emb_type] # empty text embedding for evaluation purposes\n",
    "unk_entries = pd.DataFrame({\"season\": [\"warm\", \"cold\"], \"dysp\": [\"unk\", \"unk\"], \"cough\": [\"unk\", \"unk\"], \"nasal\": [\"unk\", \"unk\"], \"BioLORD emb\": 2*[empty_text_emb]})\n",
    "all_combos = pd.DataFrame({\"season\": [\"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\"], \n",
    "                           \"dysp\": [\"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"],\n",
    "                           \"cough\": [\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\"],\n",
    "                           \"nasal\": [\"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\"], \"BioLORD emb\": 16*[empty_text_emb]})\n",
    "test_df_ext = pd.concat([test_df, unk_entries, all_combos], axis=0) # ensure correct encoding (test set does not normally contain nans) -> are removed again in TextSymptomDataset\n",
    "\n",
    "# create feature representations for train and test set\n",
    "emb_type = \"BioLORD emb\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # put train and test data on the device\n",
    "train_set = TextSymptomDataset(train_df, -1, class_map, emb_type, [\"season\", \"dysp\", \"cough\", \"nasal\"], interactions=interactions, device=device)\n",
    "test_set = TextSymptomDataset(test_df_ext, 1000, class_map, emb_type, [\"season\", \"dysp\", \"cough\", \"nasal\"], interactions=interactions, device=device)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for seed in seeds: \n",
    "    results[seed] = {}\n",
    "\n",
    "    # train diagnosis classifier\n",
    "    train_loss, test_loss, model = train_diag_classifier(train_set, test_set, diag_name, n_emb, hidden_dim, dropout, bs_train=bs_train, epochs=epochs, seed=seed, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # predict P(diag|evidence) for test set, with various selections of evidence\n",
    "    unk_features = torch.tensor([[1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "            0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # encoding of unk features when season = warm\n",
    "            [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
    "            0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
    "            0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]) # encoding of unk features when season = cold\n",
    "\n",
    "    # P(inf=yes|background,symptoms,text)\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100)\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) \n",
    "    results[seed][f\"P({diag_name}|season,symptoms,text) test PR\"] = ap\n",
    "\n",
    "    # P(inf=yes|background,symptoms)\n",
    "    empty_text_emb = train_df[train_df[\"text\"] == \"\"].iloc[0][emb_type] # empty text embedding for evaluation purposes\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100, excl_text=True, empty_text_emb=torch.tensor(empty_text_emb))\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) \n",
    "    results[seed][f\"P({diag_name}|season,symptoms) test PR\"] = ap\n",
    "\n",
    "    # P(inf=yes|background,text)\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100, excl_sympt=True)\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) # calculate average precision for these predictions, comparing with ground truth diagnosis values\n",
    "    results[seed][f\"P({diag_name}|season,text) test PR\"] = ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{422: {'P(inf|season,symptoms,text) test PR': 0.9064023507732217,\n",
       "  'P(inf|season,symptoms) test PR': 0.8811287849271705,\n",
       "  'P(inf|season,text) test PR': 0.884546608253735},\n",
       " 957: {'P(inf|season,symptoms,text) test PR': 0.9059643432904488,\n",
       "  'P(inf|season,symptoms) test PR': 0.8808977461927195,\n",
       "  'P(inf|season,text) test PR': 0.8822361432254402},\n",
       " 267: {'P(inf|season,symptoms,text) test PR': 0.9017053513420118,\n",
       "  'P(inf|season,symptoms) test PR': 0.881056510415434,\n",
       "  'P(inf|season,text) test PR': 0.8803060505952348},\n",
       " 956: {'P(inf|season,symptoms,text) test PR': 0.9046857932215641,\n",
       "  'P(inf|season,symptoms) test PR': 0.881056510415434,\n",
       "  'P(inf|season,text) test PR': 0.8816843166468447},\n",
       " 781: {'P(inf|season,symptoms,text) test PR': 0.9026494900326892,\n",
       "  'P(inf|season,symptoms) test PR': 0.8811140819639608,\n",
       "  'P(inf|season,text) test PR': 0.8816974152271206}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import performance_metrics\n",
    "\n",
    "seeds = [422, 957, 267, 956, 781]\n",
    "\n",
    "# hyperparameter settings\n",
    "interactions = False\n",
    "diag_name = \"pneu\"\n",
    "if interactions:\n",
    "    n_emb = 768+101\n",
    "else: \n",
    "    n_emb = 768+11\n",
    "hidden_dim = [256, 1]\n",
    "dropout = 0.7\n",
    "bs_train = 256\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "weight_decay = 1e-3\n",
    "\n",
    "# extend test set with all possible symptom combinations\n",
    "empty_text_emb = train_df[train_df[\"text\"] == \"\"].iloc[0][emb_type] # empty text embedding for evaluation purposes\n",
    "unk_entries = pd.DataFrame({\"season\": [\"warm\", \"cold\"], \"dysp\": [\"unk\", \"unk\"], \"cough\": [\"unk\", \"unk\"], \"nasal\": [\"unk\", \"unk\"], \"BioLORD emb\": 2*[empty_text_emb]})\n",
    "all_combos = pd.DataFrame({\"season\": [\"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"warm\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\", \"cold\"], \n",
    "                           \"dysp\": [\"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"],\n",
    "                           \"cough\": [\"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\"],\n",
    "                           \"nasal\": [\"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\"], \"BioLORD emb\": 16*[empty_text_emb]})\n",
    "test_df_ext = pd.concat([test_df, unk_entries, all_combos], axis=0) # ensure correct encoding (test set does not normally contain nans) -> are removed again in TextSymptomDataset\n",
    "\n",
    "# create feature representations for train and test set\n",
    "emb_type = \"BioLORD emb\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # put train and test data on the device\n",
    "train_set = TextSymptomDataset(train_df, -1, class_map, emb_type, [\"season\", \"dysp\", \"cough\", \"nasal\"], interactions=interactions, device=device)\n",
    "test_set = TextSymptomDataset(test_df_ext, 1000, class_map, emb_type, [\"season\", \"dysp\", \"cough\", \"nasal\"], interactions=interactions, device=device)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for seed in seeds: \n",
    "    results[seed] = {}\n",
    "\n",
    "    # train diagnosis classifier\n",
    "    train_loss, test_loss, model = train_diag_classifier(train_set, test_set, diag_name, n_emb, hidden_dim, dropout, bs_train=bs_train, epochs=epochs, seed=seed, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # predict P(diag|evidence) for test set, with various selections of evidence\n",
    "    unk_features = torch.tensor([[1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "            0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], # encoding of unk features when season = warm\n",
    "            [0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
    "            0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
    "            0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]) # encoding of unk features when season = cold\n",
    "\n",
    "    # P(pneu=yes|background,symptoms,text)\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100)\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) \n",
    "    results[seed][f\"P({diag_name}|season,symptoms,text) test PR\"] = ap\n",
    "\n",
    "    # P(pneu=yes|background,symptoms)\n",
    "    empty_text_emb = train_df[train_df[\"text\"] == \"\"].iloc[0][emb_type] # empty text embedding for evaluation purposes\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100, excl_text=True, empty_text_emb=torch.tensor(empty_text_emb))\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) \n",
    "    results[seed][f\"P({diag_name}|season,symptoms) test PR\"] = ap\n",
    "\n",
    "    # P(pneu=yes|background,text)\n",
    "    pred_df = predict_diagnoses(model, diag_name, test_set, unk_features, 100, excl_sympt=True)\n",
    "    _, ap = performance_metrics(pred_df, diag_name, model_type=\"ff\", plot=False) # calculate average precision for these predictions, comparing with ground truth diagnosis values\n",
    "    results[seed][f\"P({diag_name}|season,text) test PR\"] = ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{422: {'P(pneu|season,symptoms,text) test PR': 0.6594485107196193,\n",
       "  'P(pneu|season,symptoms) test PR': 0.10604823419110285,\n",
       "  'P(pneu|season,text) test PR': 0.6361288417163994},\n",
       " 957: {'P(pneu|season,symptoms,text) test PR': 0.642421513640098,\n",
       "  'P(pneu|season,symptoms) test PR': 0.10849483975931845,\n",
       "  'P(pneu|season,text) test PR': 0.6124411621296232},\n",
       " 267: {'P(pneu|season,symptoms,text) test PR': 0.6741234250542154,\n",
       "  'P(pneu|season,symptoms) test PR': 0.10729524870502026,\n",
       "  'P(pneu|season,text) test PR': 0.6420120238327062},\n",
       " 956: {'P(pneu|season,symptoms,text) test PR': 0.6639545456470064,\n",
       "  'P(pneu|season,symptoms) test PR': 0.10631460627670511,\n",
       "  'P(pneu|season,text) test PR': 0.6137191741098247},\n",
       " 781: {'P(pneu|season,symptoms,text) test PR': 0.646584904387248,\n",
       "  'P(pneu|season,symptoms) test PR': 0.10823646873918542,\n",
       "  'P(pneu|season,text) test PR': 0.6063633541805298}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bn-text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
